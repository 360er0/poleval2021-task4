{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download wikipedia dump\n",
    "! wget https://dumps.wikimedia.org/plwiki/20210801/plwiki-20210801-pages-articles-multistream.xml.bz2\n",
    "\n",
    "# extract raw passages with wikiextractor\n",
    "! pip install wikiextractor\n",
    "! mkdir wiki/\n",
    "! wikiextractor -o wiki/ -b 50M plwiki-20210801-pages-articles-multistream.xml.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "from copy import copy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_part(part_path):\n",
    "    data = []\n",
    "    with open(part_path) as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith('<doc '):\n",
    "                m = re.match(r'<doc id=\"(.*)\" url=\"(.*)\" title=\"(.*)\">', line)\n",
    "                data.append({\n",
    "                    'id': m.group(1),\n",
    "                    'url': m.group(2),\n",
    "                    'title': m.group(3),\n",
    "                    'content': [],\n",
    "                })\n",
    "            elif line.startswith('</doc>'):\n",
    "                continue\n",
    "            else:\n",
    "                data[-1]['content'].append(line)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_content(data, min_tokens=10, max_tokens=250):\n",
    "    out = []\n",
    "    for article in data:\n",
    "        filtered_article = copy(article)\n",
    "        filtered_article['content'] = []\n",
    "        \n",
    "        for i, line in enumerate(article['content']):\n",
    "            ls = line.split(' ')\n",
    "            if i < 2:\n",
    "                continue\n",
    "            elif i == 2 or len(ls) >= min_tokens:\n",
    "                filtered_article['content'].append(' '.join(ls[:max_tokens]))\n",
    "\n",
    "        out.append(filtered_article)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_content(data):\n",
    "    out = []\n",
    "    for article in data:\n",
    "        for i, line in enumerate(article['content']):\n",
    "            out.append({\n",
    "                'id': article['id'],\n",
    "                'url': article['url'],\n",
    "                'title': article['title'],\n",
    "                'idx': i,\n",
    "                'text': line,\n",
    "            })\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_texts(texts, multi=True, normalize=True):\n",
    "    if multi:\n",
    "        pool = encoder.start_multi_process_pool()\n",
    "        emb = encoder.encode_multi_process(texts, pool)\n",
    "        encoder.stop_multi_process_pool(pool)\n",
    "    else:\n",
    "        emb = encoder.encode(texts, convert_to_numpy=True)\n",
    "    \n",
    "    if normalize:\n",
    "        emb = emb / np.sqrt(np.sum(emb**2, axis=1, keepdims=True))\n",
    "    \n",
    "    return emb\n",
    "\n",
    "def encode_paragraph(paragraphs):\n",
    "    passages = [r['title'] + ' | ' + r['text'] for r in paragraphs]\n",
    "    embeddings = encode_texts(passages)\n",
    "    \n",
    "    out = []\n",
    "    for row, emb in zip(paragraphs, embeddings):\n",
    "        row = copy(row)\n",
    "        row['emb'] = emb\n",
    "        out.append(row)\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(part_path):\n",
    "    print(part_path)\n",
    "    \n",
    "    parsed_articles = load_part(part_path)\n",
    "    print(sum([len(a['content']) for a in parsed_articles]))\n",
    "\n",
    "    filtered_articles = filter_content(parsed_articles)\n",
    "    print(sum([len(a['content']) for a in filtered_articles]))\n",
    "    \n",
    "    paragraphs = flat_content(filtered_articles)\n",
    "    print(len(paragraphs))\n",
    "    \n",
    "    encoded_paragraphs = encode_paragraph(paragraphs)\n",
    "    print(len(encoded_paragraphs))\n",
    "    \n",
    "    with open(part_path + '.pkl', 'wb') as f:\n",
    "        pickle.dump(encoded_paragraphs, f)\n",
    "\n",
    "\n",
    "def parse_txt(part_path):\n",
    "    print(part_path)\n",
    "    \n",
    "    paragraphs = []\n",
    "    with open(part_path) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            paragraphs.append({\n",
    "                'id': part_path + '::' + str(i),\n",
    "                'url': part_path + '::' + str(i),\n",
    "                'title': 'Przys≈Çowie',\n",
    "                'idx': 0,\n",
    "                'text': line.strip(),\n",
    "            })\n",
    "    print(len(paragraphs))\n",
    "    \n",
    "    encoded_paragraphs = encode_paragraph(paragraphs)\n",
    "    print(len(encoded_paragraphs))\n",
    "    \n",
    "    with open(part_path + '.pkl', 'wb') as f:\n",
    "        pickle.dump(encoded_paragraphs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = SentenceTransformer('piotr-rybak/poleval2021-task4-herbert-large-encoder')\n",
    "\n",
    "for file in os.listdir('wiki/'):\n",
    "    parse(f'wiki/{file}')"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-9.m75",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m75"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
